{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                   TEAM NAME: DEEP-LEARNERS\n",
    "MEMBER 1 : RITIK AGARWAL <br/>\n",
    "MEMBER 2 : KESHAV BAGARIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 1 : Importing Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOSA-XaxV9s0",
    "outputId": "7a26a528-8f6a-4187-ff24-a3d32e81ff09"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.3.0.post0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.18.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: catboost in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.24.3)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catboost) (1.0.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catboost) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catboost) (1.18.1)\n",
      "Requirement already satisfied: plotly in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catboost) (4.12.0)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catboost) (1.4.1)\n",
      "Requirement already satisfied: graphviz in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catboost) (0.15)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catboost) (3.1.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.24.0->catboost) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from plotly->catboost) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->catboost) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->catboost) (2.4.6)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (45.2.0.post20200210)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: lightgbm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.1.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (0.22.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (1.18.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: wheel in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from lightgbm) (0.34.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn!=0.22.0->lightgbm) (0.14.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: vecstack in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from vecstack) (0.22.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from vecstack) (1.18.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from vecstack) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn>=0.18->vecstack) (0.14.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install catboost\n",
    "!pip install lightgbm\n",
    "!pip install vecstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Un-bEI3FGVlW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Reading the datasets and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XCmS0JIfWBQW"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_age_dataset.csv')\n",
    "test=pd.read_csv('test_age_dataset.csv')\n",
    "train.drop(columns='Unnamed: 0',inplace=True)\n",
    "test.drop(columns='Unnamed: 0',inplace=True)\n",
    "train.loc[train['gender']==2,'gender']=0\n",
    "train.loc[train['tier']==3,'tier']=0\n",
    "train.loc[train['age_group']==4,'age_group']=0\n",
    "test.loc[test['gender']==2,'gender']=0\n",
    "test.loc[test['tier']==3,'tier']=0\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(train[['tier']]).toarray())\n",
    "enc_df.drop(columns=[0],inplace=True)\n",
    "train.drop(columns=['tier'],inplace=True)\n",
    "enc_df=enc_df.rename(columns={1: \"Tier1\", 2: \"Tier2\"})\n",
    "train=train.join(enc_df)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(test[['tier']]).toarray())\n",
    "enc_df.drop(columns=[0],inplace=True)\n",
    "test.drop(columns=['tier'],inplace=True)\n",
    "enc_df=enc_df.rename(columns={1: \"Tier1\", 2: \"Tier2\"})\n",
    "test=test.join(enc_df)\n",
    "test['Tier1']=test['Tier1'].astype('int64')\n",
    "test['Tier2']=test['Tier2'].astype('int64')\n",
    "train=train.rename(columns={\"Unnamed: 0\":\"Unn\"})\n",
    "test=test.rename(columns={\"Unnamed: 0\":\"Unn\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wcLuDmxzWRWp"
   },
   "outputs": [],
   "source": [
    "X=train.copy()\n",
    "X.drop(columns=['age_group'],inplace=True)\n",
    "X['Tier1']=X['Tier1'].astype('int64')\n",
    "X['Tier2']=X['Tier2'].astype('int64')\n",
    "Y=train['age_group']\n",
    "test['Tier1']=test['Tier1'].astype('int64')\n",
    "test['Tier2']=test['Tier2'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_ilS6kV1hsY",
    "outputId": "67d20514-63d8-4e6f-84bd-45b3922dfe5e"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EK3B4EPbIpeb"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "WF9A9LtJWXAn",
    "outputId": "59eee300-8e75-44b8-dc76-5e8416833b25"
   },
   "outputs": [],
   "source": [
    "class_weigh=dict({0:2.01008585,1: 0.39641033, 2:2.05912307, 3:2.02336352})\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "paramss={'loss_function': 'MultiClassOneVsAll',\n",
    "    'learning_rate': 0.10161857081178981,\n",
    "    'l2_leaf_reg': 1,\n",
    "    'depth': 7,\n",
    "    'boosting_type': 'Plain',\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'custom_metric': 'TotalF1',\n",
    "    'task_type': 'GPU',\n",
    "    'devices': '0:1',\n",
    "    'subsample': 0.95149952752401,\n",
    "    'early_stopping_rounds':100}\n",
    "params2={'weights': 'distance', 'n_neighbors': int(27), 'algorithm': 'kd_tree', 'leaf_size': 90, 'metric': 'manhattan','n_jobs': -1}    \n",
    "models = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JblSjvdemh5w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from vecstack import stacking\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Building, Defining our Inner Models for Stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gzlZIhu93gBG"
   },
   "outputs": [],
   "source": [
    "estima=[\n",
    "    ('mee', AdaBoostClassifier(RandomForestClassifier( n_jobs=-1 ))),\n",
    "        ('8gg',BaggingClassifier(KNeighborsClassifier(n_jobs=-1),max_samples=0.5, max_features=0.5)),\n",
    "    # ('mds', BalancedBaggingClassifier(n_jobs=-1)) ,    \n",
    "  ('ms', AdaBoostClassifier()),\n",
    "\n",
    "('rm',BaggingClassifier(n_estimators=150,n_jobs=-1 ))\n",
    ",\n",
    "('m7',HistGradientBoostingClassifier( )),      \n",
    "        \n",
    "  ('mr', RandomForestClassifier(random_state=0, n_jobs=-1, \n",
    "                           n_estimators=300, max_depth=6,class_weight=class_weigh)),\n",
    "\n",
    "('m24', KNeighborsClassifier(**params2)),\n",
    "\n",
    "\n",
    "#xgbb = Pipeline([('m',XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    " #                max_depth=6,   eta= 0.05, objective= 'multi:softprob',weight=class_weigh, eval_metric='mlogloss',tree_method='gpu_hist',alpha=6,colsample_bytree=1,gpu_id=0, gamma=0.6, min_child_weight=7, n_estimators=250, sub_sample=0.8,num_class=4))])\n",
    "#models.append(('xgboost',xgbb))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "('mf', KNeighborsClassifier(n_neighbors=9,n_jobs=-1)),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "('mg',lightgbm.LGBMClassifier(task='train',boosting_type='gbdt',objective= 'multiclass',num_class= 4,metric= ['multi_logloss'],\n",
    "    learning_rate= 0.05,num_leaves= 60,max_depth= 9,feature_fraction= 0.45,class_weight=class_weigh,bagging_fraction= 0.3,reg_alpha= 0.15,reg_lambda= 0.15,min_child_weight= 0,n_jobs=-1)),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 : Fitting Our Stacking Transformer (L-0) Model on whole dataset and Storing the Out of Fold Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nH8OlK_oDPZn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [4]\n",
      "metric:       [log_loss]\n",
      "variant:      [A]\n",
      "n_estimators: [9]\n",
      "\n",
      "estimator  0: [mee: AdaBoostClassifier]\n",
      "    fold  0:  [0.50957393]\n",
      "    fold  1:  [0.50664410]\n",
      "    fold  2:  [0.50846597]\n",
      "    fold  3:  [0.50915484]\n",
      "    fold  4:  [0.51160968]\n",
      "    ----\n",
      "    MEAN:     [0.50908970] + [0.00160991]\n",
      "\n",
      "estimator  1: [8gg: BaggingClassifier]\n",
      "    fold  0:  [0.74397624]\n",
      "    fold  1:  [0.77630952]\n",
      "    fold  2:  [0.77005905]\n",
      "    fold  3:  [0.75016956]\n",
      "    fold  4:  [0.84908131]\n",
      "    ----\n",
      "    MEAN:     [0.77791913] + [0.03755153]\n",
      "\n",
      "estimator  2: [ms: AdaBoostClassifier]\n",
      "    fold  0:  [1.13602048]\n",
      "    fold  1:  [1.13969499]\n",
      "    fold  2:  [1.13586168]\n",
      "    fold  3:  [1.15620510]\n",
      "    fold  4:  [1.13612442]\n",
      "    ----\n",
      "    MEAN:     [1.14078133] + [0.00784383]\n",
      "\n",
      "estimator  3: [rm: BaggingClassifier]\n",
      "    fold  0:  [0.45722726]\n",
      "    fold  1:  [0.45734141]\n",
      "    fold  2:  [0.45890620]\n",
      "    fold  3:  [0.45759216]\n",
      "    fold  4:  [0.46040100]\n",
      "    ----\n",
      "    MEAN:     [0.45829360] + [0.00121262]\n",
      "\n",
      "estimator  4: [m7: HistGradientBoostingClassifier]\n",
      "    fold  0:  [0.47492634]\n",
      "    fold  1:  [0.47486828]\n",
      "    fold  2:  [0.47424245]\n",
      "    fold  3:  [0.47463154]\n",
      "    fold  4:  [0.67659406]\n",
      "    ----\n",
      "    MEAN:     [0.51505254] + [0.08077112]\n",
      "\n",
      "estimator  5: [mr: RandomForestClassifier]\n",
      "    fold  0:  [0.71586432]\n",
      "    fold  1:  [0.71549090]\n",
      "    fold  2:  [0.71146613]\n",
      "    fold  3:  [0.71402536]\n",
      "    fold  4:  [0.72022796]\n",
      "    ----\n",
      "    MEAN:     [0.71541494] + [0.00285996]\n",
      "\n",
      "estimator  6: [m24: KNeighborsClassifier]\n",
      "    fold  0:  [0.91836772]\n",
      "    fold  1:  [0.91804407]\n",
      "    fold  2:  [0.91220579]\n",
      "    fold  3:  [0.90579108]\n",
      "    fold  4:  [0.91446651]\n",
      "    ----\n",
      "    MEAN:     [0.91377504] + [0.00460438]\n",
      "\n",
      "estimator  7: [mf: KNeighborsClassifier]\n",
      "    fold  0:  [2.27360561]\n",
      "    fold  1:  [2.25207996]\n",
      "    fold  2:  [2.29302216]\n",
      "    fold  3:  [2.28613634]\n",
      "    fold  4:  [2.27866636]\n",
      "    ----\n",
      "    MEAN:     [2.27670208] + [0.01396457]\n",
      "\n",
      "estimator  8: [mg: LGBMClassifier]\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n",
      "    fold  0:  [0.56101423]\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n",
      "    fold  1:  [0.56194080]\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n",
      "    fold  2:  [0.55856469]\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n",
      "    fold  3:  [0.56172365]\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n",
      "    fold  4:  [0.56492627]\n",
      "    ----\n",
      "    MEAN:     [0.56163393] + [0.00203696]\n",
      "\n",
      "Train set was detected.\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [mee: AdaBoostClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [8gg: BaggingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [ms: AdaBoostClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  3: [rm: BaggingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  4: [m7: HistGradientBoostingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  5: [mr: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  6: [m24: KNeighborsClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  7: [mf: KNeighborsClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  8: [mg: LGBMClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import StackingTransformer\n",
    "stack1 = StackingTransformer(estima,regression=False , needs_proba=True, n_folds=5, stratified=True,shuffle=True, random_state=42,verbose=2 )\n",
    "\n",
    "# Fit\n",
    "stack1 = stack1.fit(X,Y)\n",
    "\n",
    "# Get your stacked features\n",
    "S_train1 = stack1.transform(X)\n",
    "#S_test = stack.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming...\n",
      "\n",
      "estimator  0: [mee: AdaBoostClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [8gg: BaggingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [ms: AdaBoostClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  3: [rm: BaggingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  4: [m7: HistGradientBoostingClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  5: [mr: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  6: [m24: KNeighborsClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  7: [mf: KNeighborsClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  8: [mg: LGBMClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    model from fold  4: done\n",
      "    ----\n",
      "    DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_test1=stack1.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzthhdhGXVzV",
    "outputId": "a2e0f0eb-70d3-43e5-838f-54a5e79df763",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54320, 36)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_test1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fitting our final estimator model on the Out-Of-Fold Predictions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=HistGradientBoostingClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EH3C0KYXWAu"
   },
   "outputs": [],
   "source": [
    "clf.fit(S_train1,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Predicting the result and saving our Final Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predss=clf.predict(s_test1)\n",
    "submit=pd.DataFrame()\n",
    "submit['prediction']=predss\n",
    "submit.loc[submit['prediction']==0,'prediction']=4\n",
    "submit.to_csv('FINALSUBMISSION.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "80.94.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
